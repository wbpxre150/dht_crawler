0# DHT Crawler Configuration File
# Lines starting with # are comments

# DHT Settings
dht_port=6881

# HTTP API Settings
http_port=8080

# Database Settings
db_path=data/torrents.db

# Logging
# Levels: DEBUG, INFO, WARN, ERROR (or 0, 1, 2, 3)
log_level=DEBUG

# ==== PHASE 2: Bloom Filter Settings ====
# Reduces database queries by 90% using probabilistic duplicate detection
bloom_enabled=1
bloom_capacity=30000000
bloom_error_rate=0.001
bloom_persist=1
bloom_path=data/bloom.dat

# ==== PHASE 4: Worker Pool Settings ====
# Increases concurrent metadata fetching for 10-20x throughput
scaling_factor=10
metadata_workers=200

# ==== PHASE 5: Batched Database Writes ====
# Improves write performance by 10-100x through transaction batching
batch_writes_enabled=1
batch_size=1000
flush_interval=60

# Meta Data Fetcher
# Maximum concurrent TCP connections globally (across all infohashes)
# Each infohash attempts 2 peers in parallel for faster metadata fetching
# This limit controls the total number of concurrent connections across all infohashes
max_concurrent_connections = 6000

# TCP connection establishment timeout in seconds
# BitTorrent peers respond quickly (<1s) or not at all
# Recommended: 2s (default was 10s)
tcp_connect_timeout_sec = 2

# Connection IDLE timeout in seconds (resets on data reception)
# Metadata transfers complete in 2-5 seconds or stall
# Recommended: 8s (default was 30s)
connection_timeout_sec = 8

# Maximum total connection lifetime in seconds
# Prevents malicious peers from holding connections indefinitely
# Recommended: 15s (default was 60s)
max_connection_lifetime_sec = 15

# Maximum metadata size in MB (reject larger)
max_metadata_size_mb = 100

# ==== wbpxre-dht Settings ====
# New DHT library with multi-threaded architecture and full BEP 51 support
wbpxre_ping_workers=20
wbpxre_find_node_workers=50
wbpxre_sample_infohashes_workers=50
wbpxre_get_peers_workers=500
wbpxre_query_timeout=6

# ==== Peer Discovery Retry Settings ====
# Retry get_peers queries multiple times to discover more peers before metadata fetch
# This dramatically improves metadata fetch success rates (5-15% â†’ 25-40%)
# Each retry queries K=8 different DHT nodes, accumulating more peers
# The get_peers worker exits after finding just 1 peer, so retries are essential
peer_retry_enabled=1
peer_retry_max_attempts=3        # Query DHT up to 4 times per info_hash
peer_retry_min_threshold=10      # Stop retrying if we have 10+ peers
peer_retry_delay_ms=300          # Wait 500ms between retry attempts
peer_retry_cleanup_interval_sec=30  # How often to cleanup stale entries (>60s old)
peer_retry_max_entries=10000     # Maximum entries before oldest is evicted

# ==== Triple Routing Table Settings ====
# Uses three rotating routing tables to eliminate race conditions and solve bootstrap problem
# - FILLING table: find_node workers insert discovered nodes here
# - STABLE table: sample_infohashes and DHT queries read from here
# - IDLE table: waiting to become the next filling table
# Key benefits:
# - Solves bootstrap problem: stable table available after first rotation (~30-60s)
# - No race conditions: strict read/write separation
# - Always have nodes for DHT queries (after bootstrap)

# Rotation threshold: trigger rotation when filling table reaches this count (default: 1500)
# Lower = faster bootstrap but more frequent rotations
# Higher = slower bootstrap but fewer rotations
# Recommended range: 1000-2000 nodes
triple_routing_threshold=1500

# Rotation time: minimum time between rotations in seconds (default: 60)
# Prevents thrashing when tables fill rapidly after bootstrap
# Also serves as the node ID rotation interval for keyspace coverage
# Lower = more keyspace exploration but more queue clearing overhead
# Higher = more stable keyspace but less exploration
# Recommended range: 60-600 seconds (1-10 minutes)
triple_routing_rotation_time=30

# ==== Pornography Content Filter ====
# Hybrid 3-layer filtering system for detecting pornographic content
# - Layer 1: Hash set keyword matching (fast pre-filter)
# - Layer 2: Regex pattern matching for evasion detection
# - Layer 3: Heuristic scoring based on multiple signals
# Expected performance: 2-7ms per torrent, 85-92% accuracy

# Enable/disable the filter (0=disabled, 1=enabled)
porn_filter_enabled=1

# Path to keyword file (relative or absolute)
porn_filter_keyword_file=porn_filter_keywords.txt

# Minimum weight for keyword match to trigger filter (1-10, default: 8)
# Higher = fewer false positives, lower = catches more variations
# OPTIMIZED: Raised to 9 to reduce false positives from generic terms
porn_filter_keyword_threshold=9

# Minimum weight for regex pattern match to trigger filter (1-10, default: 9)
# Regex patterns detect evasion techniques (l33tspeak, etc.)
porn_filter_regex_threshold=10

# Minimum heuristic score to trigger filter (0-20, default: 5)
# Heuristic scoring considers file types, naming patterns, etc.
# OPTIMIZED: Raised to 7 to reduce false positives from normal video content
porn_filter_heuristic_threshold=7

use_thread_trees = 1
num_trees = 4
# Minimum metadata rate before restarting tree (set to 0 to disable rate monitoring)
# Lowered from 0.5 to 0.2 to be less aggressive
min_metadata_rate = 0.01

# Rate monitor settings - how often to check and how long to wait before restarting
# Increased from defaults (10s check, 30s grace) to be much less aggressive
tree_rate_check_interval_sec = 60      # Check rate every 60 seconds (was 10)
tree_rate_grace_period_sec = 30        # Wait 5 minutes before restart (was 30s)

# Global bootstrap settings (NEW - replaces per-tree bootstrap)
# Target number of nodes to collect in shared pool (default: 5000)
global_bootstrap_target = 5000
# Maximum time for global bootstrap in seconds (default: 60)
global_bootstrap_timeout_sec = 120
# Number of worker threads for global bootstrap (default: 50)
global_bootstrap_workers = 50
# Number of nodes each tree samples from pool (default: 1000)
per_tree_sample_size = 1000

# Thread tree worker counts (per tree)
# find_node workers continuously discover nodes for routing table (default: 10)
# Phase 4: Reduced from 5 to 3 to further reduce CPU load
# 3 workers per tree * 4 trees = 12 total workers discovering ~30-60 nodes/sec
# This is sufficient for maintaining 1200-1500 nodes per routing table
tree_find_node_workers = 5
# BEP51 sample_infohashes workers (default: 10)
# Reduced from 20 to 10 to prevent queue overflow and improve response rate
tree_bep51_workers = 10
# Infohash queue capacity per tree (default: 5000)
# Increased from 5000 to 20000 to handle burst of samples from BEP51 responses
tree_infohash_queue_capacity = 20000
# get_peers workers (default: 500)
tree_get_peers_workers = 250
tree_metadata_workers = 300
