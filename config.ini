# DHT Crawler Configuration File - ANDROID OPTIMIZED
# Optimized for Termux/Android thread limits while maintaining throughput
# Lines starting with # are comments

# DHT Settings
dht_port=6881

# HTTP API Settings
http_port=8080

# Database Settings
db_path=data/torrents.db

# Logging
# Levels: DEBUG, INFO, WARN, ERROR (or 0, 1, 2, 3)
log_level=INFO

# ==== PHASE 2: Bloom Filter Settings ====
# Reduces database queries by 90% using probabilistic duplicate detection
bloom_enabled=1
bloom_capacity=30000000
# Failure bloom filter capacity (first-attempt failures)
# Should match main bloom capacity for consistency
failure_bloom_capacity=30000000
bloom_error_rate=0.001
bloom_persist=1
bloom_path=data/bloom.dat

# ==== PHASE 4: Worker Pool Settings ====
# Increases concurrent metadata fetching for 10-20x throughput
scaling_factor=10
metadata_workers=200

# ==== PHASE 5: Batched Database Writes ====
# Improves write performance by 10-100x through transaction batching
batch_writes_enabled=1
batch_size=500
flush_interval=60

# Meta Data Fetcher
# Maximum concurrent TCP connections globally (across all infohashes)
# Each infohash attempts 2 peers in parallel for faster metadata fetching
# This limit controls the total number of concurrent connections across all infohashes
# ANDROID: Reduced from 6000 to prevent file descriptor exhaustion
max_concurrent_connections = 5000

# TCP connection establishment timeout in seconds
# BitTorrent peers respond quickly (<1s) or not at all
# Recommended: 2s (default was 10s)
tcp_connect_timeout_sec = 2

# Connection IDLE timeout in seconds (resets on data reception)
# Metadata transfers complete in 2-5 seconds or stall
# Recommended: 8s (default was 30s)
connection_timeout_sec = 8

# Maximum total connection lifetime in seconds
# Prevents malicious peers from holding connections indefinitely
# Recommended: 15s (default was 60s)
max_connection_lifetime_sec = 15

# Maximum metadata size in MB (reject larger)
max_metadata_size_mb = 100

# ==== wbpxre-dht Settings ====
# New DHT library with multi-threaded architecture and full BEP 51 support
wbpxre_ping_workers=20
wbpxre_find_node_workers=50
wbpxre_sample_infohashes_workers=50
wbpxre_get_peers_workers=500
wbpxre_query_timeout=6

# ==== Peer Discovery Retry Settings ====
# Retry get_peers queries multiple times to discover more peers before metadata fetch
# This dramatically improves metadata fetch success rates (5-15% → 25-40%)
# Each retry queries K=8 different DHT nodes, accumulating more peers
# The get_peers worker exits after finding just 1 peer, so retries are essential
peer_retry_enabled=1
peer_retry_max_attempts=3        # Query DHT up to 4 times per info_hash
peer_retry_min_threshold=10      # Stop retrying if we have 10+ peers
peer_retry_delay_ms=300          # Wait 500ms between retry attempts
peer_retry_cleanup_interval_sec=30  # How often to cleanup stale entries (>60s old)
peer_retry_max_entries=10000     # Maximum entries before oldest is evicted

# ==== Triple Routing Table Settings ====
# Uses three rotating routing tables to eliminate race conditions and solve bootstrap problem
# - FILLING table: find_node workers insert discovered nodes here
# - STABLE table: sample_infohashes and DHT queries read from here
# - IDLE table: waiting to become the next filling table
# Key benefits:
# - Solves bootstrap problem: stable table available after first rotation (~30-60s)
# - No race conditions: strict read/write separation
# - Always have nodes for DHT queries (after bootstrap)

# Rotation threshold: trigger rotation when filling table reaches this count (default: 1500)
# Lower = faster bootstrap but more frequent rotations
# Higher = slower bootstrap but fewer rotations
# Recommended range: 1000-2000 nodes
triple_routing_threshold=1500

# Rotation time: minimum time between rotations in seconds (default: 60)
# Prevents thrashing when tables fill rapidly after bootstrap
# Also serves as the node ID rotation interval for keyspace coverage
# Lower = more keyspace exploration but more queue clearing overhead
# Higher = more stable keyspace but less exploration
# Recommended range: 60-600 seconds (1-10 minutes)
triple_routing_rotation_time=30

# ==== Pornography Content Filter ====
# Hybrid 3-layer filtering system for detecting pornographic content
# - Layer 1: Hash set keyword matching (fast pre-filter)
# - Layer 2: Regex pattern matching for evasion detection
# - Layer 3: Heuristic scoring based on multiple signals
# Expected performance: 2-7ms per torrent, 85-92% accuracy

# Enable/disable the filter (0=disabled, 1=enabled)
porn_filter_enabled=1

# Path to keyword file (relative or absolute)
porn_filter_keyword_file=porn_filter_keywords.txt

# Minimum weight for keyword match to trigger filter (1-10, default: 8)
# Higher = fewer false positives, lower = catches more variations
# OPTIMIZED: Raised to 9 to reduce false positives from generic terms
porn_filter_keyword_threshold=9

# Minimum weight for regex pattern match to trigger filter (1-10, default: 9)
# Regex patterns detect evasion techniques (l33tspeak, etc.)
porn_filter_regex_threshold=10

# Minimum heuristic score to trigger filter (0-20, default: 5)
# Heuristic scoring considers file types, naming patterns, etc.
# OPTIMIZED: Raised to 7 to reduce false positives from normal video content
porn_filter_heuristic_threshold=7

# ==== ANDROID OPTIMIZATION: Thread Tree Architecture ====
# Strategy: MORE trees with FEWER workers each = same parallelism, fewer threads
# Original: 16 trees × ~1365 threads/tree = ~21,840 threads (FAILS on Android)
# Android:  32 trees × ~300 threads/tree = ~9,600 threads (WORKS on Android)

use_thread_trees = 1

# ANDROID: Double the tree count to compensate for reduced workers
# More trees = better parallelism without excessive threads per tree
num_trees = 32

# Global bootstrap settings (NEW - replaces per-tree bootstrap)
# Target number of nodes to collect in shared pool (default: 5000)
global_bootstrap_target = 5000
# Maximum time for global bootstrap in seconds (default: 60)
global_bootstrap_timeout_sec = 120
# ANDROID: Reduced from 50 to 30 to save threads
global_bootstrap_workers = 30
# Number of nodes each tree samples from pool (default: 1000)
per_tree_sample_size = 1000

# BEP51 Node Cache Settings
# Cache file path for persistent BEP51-capable nodes
bep51_cache_path = data/bep51_cache.dat
# Maximum number of nodes to cache (default: 10000)
# When cache is FULL, it becomes the PRIMARY bootstrap source for thread trees
# This provides better node quality than the static shared pool
bep51_cache_capacity = 5000
# Percentage of BEP51 responses to cache (1-100, default: 5)
bep51_cache_submit_percent = 5

# BEP51 per-node query cooldown in seconds (default: 30)
# Prevents flooding the same BEP51-capable nodes with queries
# Higher = less rate-limiting but slower discovery
# Lower = faster queries but risk of being banned
bep51_node_cooldown_sec = 30

# ==== ANDROID: Thread tree worker counts (per tree) ====
# CRITICAL: These are multiplied by num_trees, so keep them LOW

# find_node workers continuously discover nodes for routing table (default: 10)
# ANDROID: Reduced from 15 to 5
# 5 workers × 32 trees = 160 total (was 240)
tree_find_node_workers = 5

# BEP51 sample_infohashes workers (default: 10)
# ANDROID: Reduced from 50 to 10
# 10 workers × 32 trees = 320 total (was 800)
tree_bep51_workers = 10

# get_peers workers (default: 500)
# ANDROID: Reduced from 1000 to 100
# 100 workers × 32 trees = 3,200 total (was 16,000!)
tree_get_peers_workers = 125

# Metadata workers per tree
# ANDROID: Reduced from 300 to 75
# 75 workers × 32 trees = 2,400 total (was 4,800!)
tree_metadata_workers = 110

# TOTAL THREADS PER TREE: 5 + 10 + 100 + 75 = 190
# TOTAL THREADS: 32 trees × 190 = 6,080 threads (plus ~100 overhead)
# FINAL: ~6,200 threads (well within Android limits)

# Infohash queue capacity per tree (default: 5000)
# Increased from 5000 to 20000 to handle burst of samples from BEP51 responses
tree_infohash_queue_capacity = 3000

# BEP51 query interval in milliseconds (default: 10ms)
# Reduced to 5ms to increase query rate with larger routing tables
tree_bep51_query_interval_ms = 5

# ==== Find_node Worker Throttling ====
# Prevents find_node workers from overwhelming CPU and causing mutex contention
# Pauses find_node workers when infohash queue is sufficiently full to give
# get_peers workers more CPU time and mutex access
# Pause find_node workers when infohash queue reaches this size (default: 2000)
tree_infohash_pause_threshold = 2500
# Resume find_node workers when infohash queue falls below this size (default: 1000)
tree_infohash_resume_threshold = 1000

# ==== Get_peers Worker Throttling ====
# Prevents get_peers workers from overwhelming metadata workers
# Pauses get_peers workers when peers queue is sufficiently full to prevent
# queue overflow and reduce system load when metadata fetching is the bottleneck

# Peers queue capacity per tree (default: 2000)
tree_peers_queue_capacity = 3000

# Pause get_peers workers when peers queue reaches this size (default: 2000)
tree_peers_pause_threshold = 2500
# Resume get_peers workers when peers queue falls below this size (default: 1000)
tree_peers_resume_threshold = 1000

# Keyspace positioning
use_keyspace_partitioning=1

# ==== Thread Tree Metadata Rate-Based Respawn ====
# Trees are respawned when metadata fetch rate falls below threshold
# Low metadata rate indicates exhausted keyspace region

# Minimum metadata rate before restarting tree (set to 0 to disable rate monitoring)
min_metadata_rate = 0.08

# Rate monitor settings - how often to check and how long to wait before restarting
tree_rate_check_interval_sec = 120
tree_rate_grace_period_sec = 60

# Thread tree minimum lifetime before rate monitoring activates (minutes)
# NOTE: Each tree gets a randomized grace period of ±33% to prevent
# synchronized respawning. For example, if set to 20 minutes, actual
# grace periods will range from ~13.3 to ~26.7 minutes across trees.
tree_min_lifetime_minutes = 20

# Only destroy trees when infohash queue is empty (0=disabled, 1=enabled)
tree_require_empty_queue = 0

# ==== Tree Respawn Overlapping Settings ====
# Reduces downtime during respawns by spawning replacement trees while old trees drain
# When a tree is marked for respawn, wait until active_connections drops below threshold,
# then spawn the replacement while the old tree finishes draining its metadata connections

# Spawn replacement tree when active connections drop to this threshold (default: 50)
# Lower = longer wait before respawn, higher = earlier respawn with more concurrent connections
# Recommended: 50 (low enough to avoid huge thread spike, high enough to trigger early)
respawn_spawn_threshold = 50

# Force destroy draining tree after this timeout in seconds (default: 120)
# Prevents draining trees from hanging indefinitely if connections stall
# Should be 3-4x the metadata timeout to handle slow peers gracefully
respawn_drain_timeout_sec = 120

# Maximum trees allowed in draining state simultaneously (default: 8)
# Controls maximum thread spike during overlapped respawns
# 8 allows 25% of trees (8/32) to be draining without runaway threads
max_draining_trees = 8

# ==== Refresh Thread Settings (for /refresh HTTP endpoint) ====
# The refresh thread maintains a small routing table for on-demand peer queries
# This is a lightweight singleton thread separate from the main crawler trees

# Number of nodes to sample from shared pool for bootstrap (default: 1000)
refresh_bootstrap_sample_size = 1000

# Target routing table size (default: 500)
# Smaller than main trees to reduce resource usage
refresh_routing_table_target = 500

# Ping workers for routing table health checks (default: 1)
refresh_ping_workers = 1

# Find_node workers for routing table maintenance (default: 1)
refresh_find_node_workers = 1

# Get_peers workers for processing refresh requests (default: 1)
# Only 1 worker since requests are processed sequentially
refresh_get_peers_workers = 1

# Request queue capacity (default: 100)
# How many pending refresh requests can be queued
refresh_request_queue_capacity = 100

# Get_peers response timeout in milliseconds (default: 500)
# NOTE: Refresh thread queries the 8 closest nodes once (no iterations)
# matching the wbpxre_dht implementation
refresh_get_peers_timeout_ms = 500
