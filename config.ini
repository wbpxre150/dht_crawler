# DHT Crawler Configuration File
# Lines starting with # are comments

# DHT Settings
dht_port=6881

# Active DHT Maintenance (Critical for peer discovery)
# These settings keep the node active in the DHT network
# CONSERVATIVE DEFAULTS: Prevents routing table churn during bootstrap
# NOTE: Active maintenance is DISABLED until 100+ good nodes (prevents eviction churn)
bucket_refresh_interval_sec=90
neighbourhood_refresh_interval_sec=60    # Every 60s (reduced from aggressive 15s)
wandering_search_interval_sec=30         # Every 30s (reduced from aggressive 5s)
wandering_searches_per_cycle=3           # 3 per cycle (reduced from aggressive 20)
max_concurrent_searches=15               # 15 concurrent (reduced from aggressive 100)

# Cache Settings
cache_enabled=0
cache_max_peers=200
cache_save_interval_sec=300

# Monitoring & Health Thresholds
routing_table_log_interval_sec=60
min_good_nodes_threshold=50              # Target minimum for healthy operation
# CRITICAL mode activates when good < 10 nodes (emergency bootstrap)
# RECOVERY mode activates when good < 50 nodes (aggressive bootstrap)

# HTTP API Settings
http_port=8080

# Database Settings
db_path=data/torrents.db

# Logging
# Levels: DEBUG, INFO, WARN, ERROR (or 0, 1, 2, 3)
log_level=DEBUG

# ==== PHASE 2: Bloom Filter Settings ====
# Reduces database queries by 90% using probabilistic duplicate detection
bloom_enabled=1
bloom_capacity=30000000
bloom_error_rate=0.001
bloom_persist=1
bloom_path=data/bloom.dat

# ==== PHASE 3: Active Network Exploration ====
# Improves DHT network coverage through active queries
# MODERATE MODE: Balanced node discovery (reduced from aggressive to prevent routing table flooding)
exploration_enabled=1
target_rotation_interval=10
find_node_rate=10                        # 10 queries/sec (was 50) - moderate discovery
find_node_workers=50                     # 50 concurrent workers (was 300) - reasonable parallelism
discovered_queue_capacity=50000          # 50K queue (scaled with routing table) - large buffer for node discovery
bootstrap_reseed_interval_sec=300        # Every 5 min (was 120) when unhealthy

# ==== PHASE 4: Worker Pool Settings ====
# Increases concurrent metadata fetching for 10-20x throughput
scaling_factor=10
metadata_workers=100

# ==== PHASE 5: Batched Database Writes ====
# Improves write performance by 10-100x through transaction batching
batch_writes_enabled=1
batch_size=1000
flush_interval=60

# ==== PHASE 6: Shadow Routing Table ====
# Tracks enhanced node metadata for better BEP 51 node selection
shadow_table_enabled=0
shadow_table_capacity=10000
shadow_table_prune_interval=600
shadow_table_persist=1
shadow_table_path=data/shadow_table.dat

# ==== PHASE 7: Active Ping Verification (BUGFIX) ====
# DISABLED: jech/dht library already handles node verification automatically
# Manual pinging interferes with jech/dht's internal node management
ping_verification_enabled=0
ping_verification_interval=5      # Ping dubious nodes every 5 seconds
ping_max_dubious_per_cycle=100    # Ping up to 100 dubious nodes per cycle
ping_dubious_age_threshold=120    # Consider nodes dubious if no response in 2 minutes

# Meta Data Fetcher
# Maximum concurrent TCP connections per info_hash
# OPTIMIZED: Reduced from 20 to 10 to avoid overwhelming peers
# With sequential peer fallback, we now try ALL available peers until success
# Lower concurrent value per hash allows better global connection distribution
concurrent_peers_per_torrent = 5

# Maximum concurrent TCP connections globally (across all infohashes)
# AGGRESSIVE: High limit to maximize throughput across all torrents
max_concurrent_connections = 2000

# Connection timeout in seconds
# OPTIMIZED: Reduced from 6s to 4s to fail faster and try next peer
# Faster timeouts mean quicker peer rotation and better success rates
connection_timeout_sec = 4

# Maximum metadata size in MB (reject larger)
max_metadata_size_mb = 100

# Retry failed metadata fetches (0=disabled, 1=enabled)
# DISABLED: With sequential peer fallback, we exhaust all peers on first attempt
# No need to retry since we already tried all available peers
# Expected success rate should improve from 5% to 15-25% with this optimization
retry_enabled = 0

# ==== wbpxre-dht Settings ====
# New DHT library with multi-threaded architecture and full BEP 51 support
wbpxre_ping_workers=20
wbpxre_find_node_workers=20
wbpxre_sample_infohashes_workers=80
wbpxre_get_peers_workers=2000
wbpxre_query_timeout=4

# Maximum nodes in routing table (default: 10000)
# Higher values improve DHT coverage but use more memory (~200 bytes per node)
# OPTIMIZED: 60000 nodes provides headroom for rotation (90% = 54K triggers eviction)
# 60000 nodes ≈ 12 MB memory
# NOTE: The maintenance thread automatically pings and removes stale nodes
# FIX: Increased from 50K to 60K to reduce eviction pressure after rotation
max_routing_table_nodes=60000

# ==== Node Health and Pruning Settings ====
# BALANCED PRUNING: Conservative settings to maintain stable routing table
# Prevents over-aggressive eviction that causes routing table churn
# CRITICAL: These settings directly impact DHT discovery rate!
max_node_age_sec=120                      # Consider nodes old after 10 minutes (was 2 min - WAY too aggressive!)
node_verification_batch_size=500          # Verify up to 200 old nodes per cycle (was 3000 - caused death spiral!)
node_cleanup_interval_sec=10              # Clean dropped nodes every 30 seconds (was 10s - too frequent)
min_node_response_rate=0.15               # Evict nodes with <15% response rate (was 25% - too strict!)
node_quality_min_queries=10               # Need 10 queries before judging quality (was 3 - not enough data!)

# ==== BEP51-Focused Pruning ====
# Aggressively removes non-BEP51 nodes when routing table exceeds min_capacity
# Goal: Maximize BEP51 nodes for optimal info_hash discovery
bep51_pruning_enabled=1                   # Enable BEP51-focused pruning (0=disabled, 1=enabled)
bep51_pruning_interval_sec=10             # How often to check and prune (default: 30 seconds)
bep51_pruning_min_capacity=0.0            # Min fill ratio to trigger pruning (0.0=always, 0.9=only when 90% full)

# ==== Node ID Rotation Settings ====
# Periodically change node ID to explore different DHT keyspace regions
# This increases metadata discovery by moving to different parts of the network
# HOT ROTATION MODE: Preserves routing table and keeps workers running (NO DOWNTIME!)
# - Eliminates ~120 second bootstrap penalty per rotation
# - Maintains 6,500+ routing table nodes across rotations
# - Three-phase transition: STABLE -> ANNOUNCING (30s) -> TRANSITIONING (30s) -> STABLE
# - Expected 3-4x improvement in discovery rate vs destructive rotation
# OPTIMIZED: 300s (5 min) rotation with aggressive post-rotation pruning
node_rotation_enabled=1
node_rotation_interval_sec=120        # Rotate every 2 minutes
clear_sample_queue_on_rotation=1      # Clear sample_infohashes queue on rotation (0=keep, 1=clear)
                                      # Clearing helps focus on new keyspace, keeping preserves in-flight work

# ==== Peer Discovery Retry Settings ====
# Retry get_peers queries multiple times to discover more peers before metadata fetch
# This dramatically improves metadata fetch success rates (5-15% → 25-40%)
# Each retry queries K=8 different DHT nodes, accumulating more peers
# The get_peers worker exits after finding just 1 peer, so retries are essential
peer_retry_enabled=1
peer_retry_max_attempts=3        # Query DHT up to 3 times per info_hash
peer_retry_min_threshold=10      # Stop retrying if we have 10+ peers
peer_retry_delay_ms=500          # Wait 500ms between retry attempts
