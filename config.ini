# DHT Crawler Configuration File
# Lines starting with # are comments

# DHT Settings
dht_port=6881

# HTTP API Settings
http_port=8080

# Database Settings
db_path=data/torrents.db

# Logging
# Levels: DEBUG, INFO, WARN, ERROR (or 0, 1, 2, 3)
log_level=INFO

# ==== PHASE 2: Bloom Filter Settings ====
# Reduces database queries by 90% using probabilistic duplicate detection
bloom_enabled=1
bloom_capacity=30000000
bloom_error_rate=0.001
bloom_persist=1
bloom_path=data/bloom.dat

# ==== PHASE 4: Worker Pool Settings ====
# Increases concurrent metadata fetching for 10-20x throughput
scaling_factor=10
metadata_workers=50

# ==== PHASE 5: Batched Database Writes ====
# Improves write performance by 10-100x through transaction batching
batch_writes_enabled=1
batch_size=1000
flush_interval=60

# Meta Data Fetcher
# Maximum concurrent TCP connections per info_hash
# OPTIMIZED: Increased to 4 for better success rates with typical 4-6% peer reachability
# With sequential peer fallback, we try ALL available peers until success
# Higher concurrent value improves probability of at least one successful connection
# (4 peers × 5% success = 20% chance per batch vs 2 peers × 5% = 10% chance)
concurrent_peers_per_torrent = 4

# Maximum concurrent TCP connections globally (across all infohashes)
# AGGRESSIVE: High limit to maximize throughput across all torrents
max_concurrent_connections = 8000

# TCP connection establishment timeout in seconds
# This timeout applies only during the TCP connection phase (before handshake)
# OPTIMIZED: Reduced to 8s to fail faster on dead/unreachable peers
# Allows rapid cycling through peer list to find reachable ones
# Recommendation: 6-10s for most cases, higher only for very slow networks
tcp_connect_timeout_sec = 8

# Connection IDLE timeout in seconds (ACTIVITY-BASED - resets on data reception)
# Connection will close if NO DATA is received for this duration
# Timer resets whenever any data is received, allowing slow transfers to complete
# OPTIMIZED: Increased to 40s to be more patient with slow metadata transfers
# This only applies AFTER successful TCP connection, so it doesn't slow down failures
# Recommendation: 30-60s for most cases, higher for very slow networks
connection_timeout_sec = 40

# Maximum total connection lifetime in seconds (0 = unlimited)
# Hard limit to prevent connections from staying open indefinitely
# This applies even if connection is actively receiving data
# Prevents edge cases where malicious/broken peers drip-feed data forever
# Recommendation: 60s is sufficient for most metadata transfers (<10MB typical)
# Set to 0 to rely only on idle timeout (not recommended)
max_connection_lifetime_sec = 60

# Maximum metadata size in MB (reject larger)
max_metadata_size_mb = 100

# ==== wbpxre-dht Settings ====
# New DHT library with multi-threaded architecture and full BEP 51 support
wbpxre_ping_workers=50
wbpxre_find_node_workers=20
wbpxre_sample_infohashes_workers=200
wbpxre_get_peers_workers=3000
wbpxre_query_timeout=5

# Maximum nodes in routing table (default: 10000)
# Higher values improve DHT coverage but use more memory (~200 bytes per node)
# OPTIMIZED: 60000 nodes provides headroom for rotation (90% = 54K triggers eviction)
# 60000 nodes ≈ 12 MB memory
# FIX: Increased from 50K to 60K to reduce eviction pressure after rotation
max_routing_table_nodes=60000

# ==== Node ID Rotation Settings ====
# Periodically change node ID to explore different DHT keyspace regions
# This increases metadata discovery by moving to different parts of the network
# HOT ROTATION MODE: Preserves routing table and keeps workers running (NO DOWNTIME!)
# - Eliminates ~120 second bootstrap penalty per rotation
# - Maintains 6,500+ routing table nodes across rotations
# - Three-phase transition: STABLE -> ANNOUNCING -> TRANSITIONING -> STABLE
# - Expected 3-4x improvement in discovery rate vs destructive rotation
# OPTIMIZED: 300s (5 min) rotation with aggressive post-rotation pruning
node_rotation_enabled=1
node_rotation_interval_sec=15         # Rotate every 30 seconds
rotation_phase_duration_sec=15        # Duration (in seconds) for ANNOUNCING and TRANSITIONING phases
clear_sample_queue_on_rotation=1      # Clear sample_infohashes queue on rotation (0=keep, 1=clear)
                                      # Clearing helps focus on new keyspace, keeping preserves in-flight work

# ==== Peer Discovery Retry Settings ====
# Retry get_peers queries multiple times to discover more peers before metadata fetch
# This dramatically improves metadata fetch success rates (5-15% → 25-40%)
# Each retry queries K=8 different DHT nodes, accumulating more peers
# The get_peers worker exits after finding just 1 peer, so retries are essential
# OPTIMIZED: Aggressive peer discovery to compensate for 4-6% peer reachability rate
# With 25 peers × 5% success rate = 1.25 expected successful connections (vs 0.5 with 10 peers)
peer_retry_enabled=1
peer_retry_max_attempts=8        # Query DHT up to 8 times per info_hash (was 4)
peer_retry_min_threshold=25      # Stop retrying if we have 25+ peers (was 10)
peer_retry_delay_ms=250          # Wait 250ms between retry attempts (was 500ms)

# ==== Async Node Pruning Settings ====
# Runs periodically in background to maintain optimal routing table health
# Combines distance-based and age-based pruning for optimal keyspace coverage
# Independent of node ID rotation (timer-based)
# Multi-threaded worker pool for 3-4x performance improvement
async_pruning_enabled=1                    # Enable async pruning (0=disabled, 1=enabled)
async_pruning_interval_sec=5             # Run pruning every X seconds (default: 120 = 2 minutes)
async_pruning_target_nodes=10000           # Target routing table size after pruning (default: 60K)
async_pruning_distant_percent=90.0         # Remove X% of distant nodes (0.0-100.0, default: 50%)
async_pruning_old_percent=90.0             # Remove Y% of oldest nodes (0.0-100.0, default: 30%)
async_pruning_batch_size=5000              # Nodes per batch for drop operations (default: 1000)
async_pruning_log_interval=10000           # Log progress every N nodes (default: 5000)
async_pruning_workers=4                    # Number of worker threads (1-16, default: 4)
async_pruning_delete_chunk_size=100        # Nodes per delete chunk - smaller = less blocking (default: 100)

# Minimum capacity threshold before pruning activates (0.0-100.0, default: 70%)
# Pruning is skipped unless routing table is at least X% of max_routing_table_nodes
# This prevents wasting CPU during bootstrap when table is still filling up
# Set lower (30-50%) for aggressive pruning, higher (80-90%) for conservative pruning
# With fast-filling tables, call pruning more often (lower interval) but only when capacity is high
async_pruning_min_capacity_percent=70.0    # Min capacity % before pruning (default: 70%)

# Hash index rebuild interval (default: 10)
# Rebuild the hash table every N pruning cycles to prevent uthash bucket array bloat
# uthash never shrinks its bucket array, causing performance degradation over time
# Rebuilding resets bucket array size to current node count, maintaining O(1) performance
# Lower values = more frequent rebuilds (safer but slightly more overhead)
# Higher values = less overhead but longer time before performance recovery
# Recommended: 10 (rebuilds every 10 pruning cycles, ~20 minutes with 2-min intervals)
async_pruning_hash_rebuild_cycles=5       # Rebuild hash index every N cycles (default: 10)
